This repo is forked from RWKV-LM-LoRA

# RWKV-5.2 LoRA Experiment RWKV5-LM-LoRA

Added LoRA training functionality to the RWKV v5.2 model.

Now you can initiate LoRA training for the RWKV-5-World model.

in 7b model,training can be performed with 24GB of VRAM if less ctx,rank

The basic commands follow those of RWKV-LM-LoRA.

Examples of training commands can be found in lora-training.sh, so please make changes as needed.



# And Thanks to:
RWKV-LM @BlinkDL
RWKV-LM-LoRA @Blealtan



# License
same with RWKV-LM and RWKV-LM-LoRA

Apache 2.0


@ 2024 OpenMOSE
